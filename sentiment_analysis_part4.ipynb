{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Alternative max-marginal decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def createDf(language,file):\n",
    "    '''\n",
    "    language = 'CN' , 'EN', 'FR', 'SG'\n",
    "    '''\n",
    "    tweets = []          # A list of all the tweets (Each tweet is a list)\n",
    "    word_count, tweet_count = 0, 0\n",
    "    # Import training data\n",
    "    with open('./' + language + file, encoding='utf8') as f:\n",
    "        training_lines = f.readlines()\n",
    "\n",
    "        # For each line in the file\n",
    "        for line in training_lines: \n",
    "\n",
    "            # If line is empty (i.e. we enter a new tweet)\n",
    "            if line in['\\n', '\\r\\n']: # Initialize a new tweet, reset word count\n",
    "                if word_count != 0: #If the previous tweet was not empty, increase tweet count\n",
    "                    tweet_count += 1\n",
    "                word_count = 0\n",
    "\n",
    "            else:\n",
    "                # Remove the spaces in each line\n",
    "                stripped = line.strip().split(\" \")\n",
    "                if len(stripped) == 2:\n",
    "                    if word_count == 0:\n",
    "                        tweets.append([tweet_count, word_count,'None','Start'])\n",
    "                        word_count += 1\n",
    "                    tweets.append([tweet_count, word_count] + stripped)\n",
    "                    word_count += 1\n",
    "                    \n",
    "    df = pd.DataFrame(tweets,columns=['Tweet', 'Word', 'Observation', 'State'])\n",
    "    df = df.set_index(['Tweet', 'Word'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDfDevin(language,file):\n",
    "    '''\n",
    "    language = 'CN' , 'EN', 'FR', 'SG'\n",
    "    '''\n",
    "    tweets = []          # A list of all the tweets (Each tweet is a list)\n",
    "    word_count, tweet_count = 0, 0\n",
    "    # Import training data\n",
    "    with open('./' + language + file, encoding='utf8') as f:\n",
    "        training_lines = f.readlines()\n",
    "\n",
    "        # For each line in the file\n",
    "        for line in training_lines: \n",
    "\n",
    "            # If line is empty (i.e. we enter a new tweet)\n",
    "            if line in['\\n', '\\r\\n']: # Initialize a new tweet, reset word count\n",
    "                if word_count != 0: #If the previous tweet was not empty, increase tweet count\n",
    "                    tweet_count += 1\n",
    "                word_count = 0\n",
    "\n",
    "            else:\n",
    "                # Remove the spaces in each line\n",
    "                stripped = line.strip().split(\" \")\n",
    "                if len(stripped) == 1:\n",
    "                    if word_count == 0:\n",
    "                        tweets.append([tweet_count, word_count,'None'])\n",
    "                        word_count += 1\n",
    "                    tweets.append([tweet_count, word_count] + stripped)\n",
    "                    word_count += 1\n",
    "                    \n",
    "    df = pd.DataFrame(tweets,columns=['Tweet', 'Word', 'Observation'])\n",
    "    df = df.set_index(['Tweet', 'Word'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweet(df, tweetNumber):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    df: dataframe of all tweets\n",
    "    tweetNumber: which tweet to access and extract\n",
    "\n",
    "    Output:\n",
    "    obs_list: list of observations for a specified tweet \n",
    "    \"\"\"\n",
    "    df_resetindex = df.reset_index()\n",
    "    tweet_df = df_resetindex.loc[df_resetindex['Tweet'] == tweetNumber]\n",
    "    \n",
    "    # Convert tweet dataframe to a list\n",
    "    tweet_list = tweet_df.values.T.tolist()\n",
    "    \n",
    "    # Append a None at the end of observation to account for 'Stop' state\n",
    "    obs_list = tweet_list[2]\n",
    "    obs_list.append('None')\n",
    "    \n",
    "    # returns a list of observations\n",
    "    return obs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_State(df):\n",
    "    '''\n",
    "    Get Count(i) and Count(j)\n",
    "    '''\n",
    "    states_counter = df.groupby('State').count()\n",
    "    return states_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Emission Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Emission(df):\n",
    "    df = df.copy()\n",
    "    df[\"Count\"] = 1\n",
    "    \n",
    "    emission_count = df.groupby(['State','Observation'],).count().reset_index(level = 'Observation')\n",
    "    emission_count = emission_count.join(Count_State(df), rsuffix = '_State')\n",
    "    emission_count = emission_count.drop('Observation_State',axis=1)    \n",
    "    emission_count[\"emission\"] = emission_count['Count'] / emission_count['Count_State']\n",
    "    emission_count = emission_count.drop('Count_State',axis=1)\n",
    "    \n",
    "    return emission_count\n",
    "\n",
    "def Replace_With_Unk(df, k):\n",
    "    emission_count = df.copy()\n",
    "    drop_table = emission_count.groupby(['Observation'],).sum()\n",
    "    drop_table = drop_table.loc[drop_table['Count'] < k].reset_index()\n",
    "    emission_count['Observation'].loc[emission_count['Observation'].isin(drop_table['Observation'])] = '#UNK#'\n",
    "    emission_count = emission_count.groupby(['State','Observation'],).sum()\n",
    "    \n",
    "    return emission_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5 pts) Write a function that estimates the transition parameters from the training set using MLE (maximum likelihood estimation):\n",
    "\n",
    "#### Please make sure the following special cases are also considered: q(STOP|yn) and q(y1|START)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/mle2.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Transistion(df):\n",
    "    transistion = df.copy()\n",
    "    transistion['J'] = transistion['State']\n",
    "    transistion['J'] = transistion['J'].shift(-1)\n",
    "    transistion['J'].loc[transistion['J'] == 'Start'] = 'Stop'\n",
    "    transistion['J'].loc[pd.isnull(transistion['J'])] = 'Stop'\n",
    "    count_transistion = transistion.groupby(['State','J']).count()\n",
    "    \n",
    "    # Create Full table of transistion permutations\n",
    "    states = Count_State(df).reset_index().as_matrix()[:-1,0]\n",
    "    length = states.shape[0] + 1\n",
    "    start = np.reshape(np.concatenate((['Start'],states)),(1,-1))\n",
    "    Stop = np.reshape(np.concatenate((states,['Stop'])),(1,-1))\n",
    "    states = np.vstack((np.repeat(start,length),np.ravel(np.repeat(Stop,length,axis=0)))).T\n",
    "    states = pd.DataFrame(states, columns=['State','J'])\n",
    "    states['Observation'] = 0\n",
    "    states = states.set_index(['State','J'])\n",
    "    count_transistion = states.join(count_transistion, how= 'left', lsuffix= '2').drop('Observation2', axis = 1).fillna(0)\n",
    "    \n",
    "    # Compute transistion probabilities\n",
    "    count_transistion = count_transistion.join(Count_State(df), lsuffix='_trans', rsuffix='_state')\n",
    "    count_transistion['aij'] = count_transistion['Observation_trans'] / count_transistion['Observation_state']\n",
    "    print(\"Transistion Dataframe created.\")\n",
    "    \n",
    "    return count_transistion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ObtainEmission(state, observation,count_emission):\n",
    "    df = count_emission.reset_index()\n",
    "    renamed = df.loc[(df['State']==state) & (df['Observation']==observation)].emission.as_matrix()\n",
    "    if len(renamed) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return renamed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def alpha(tweet,j,count_state,count_emission,count_transistion,order,all_words,alpha_list,n_states):\n",
    "    n = len(tweet)\n",
    "    \n",
    "    # Base case\n",
    "    if j == 1:\n",
    "        a1 = [count_transistion.get(key) for key in [('Start',state) for state in order]]\n",
    "        a1 = np.asarray(a1)\n",
    "        alpha_list.append(a1)\n",
    "        return alpha(tweet,2,count_state,count_emission,count_transistion,order,all_words,alpha_list,n_states)\n",
    "    \n",
    "    # Final case\n",
    "    elif j == n-1:\n",
    "        return (alpha_list)\n",
    "    \n",
    "    # Recursive case\n",
    "    else:\n",
    "        a = []\n",
    "        \n",
    "        # Replace unknown words with #UNK#\n",
    "        if tweet[j-1] not in all_words:\n",
    "            tweet[j-1] = '#UNK#'\n",
    "            \n",
    "        for i in range(n_states):\n",
    "            a_transition = [count_transistion.get(key) for key in [(state,order[i]) for state in order]]\n",
    "            a_transition = np.asarray(a_transition)\n",
    "            a_emission = [0 if i is None else i for i in [count_emission.get(key)\\\n",
    "                                         for key in [(state,tweet[j-1]) for state in order]]]\n",
    "            a_emission = np.asarray(a_emission)\n",
    "            previous_a = alpha_list[len(alpha_list)-1]\n",
    "\n",
    "            summation = np.multiply(previous_a,a_transition)\n",
    "            summation = np.multiply(a_emission,summation)\n",
    "\n",
    "            a.append(np.sum(summation))\n",
    "            \n",
    "        alpha_list.append(a)\n",
    "        \n",
    "        return alpha(tweet,j+1,count_state,count_emission,count_transistion,order,all_words,alpha_list,n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta(tweet,k,count_state,count_emission,count_transistion,order,all_words,beta_list,n_states):\n",
    "    n = len(tweet)\n",
    "    \n",
    "    # Base case\n",
    "    if k == -1 :\n",
    "        k = n-1\n",
    "        b_transition = [count_transistion.get(key) for key in [(state,'Stop') for state in order]]\n",
    "        \n",
    "        if tweet[k-1] not in all_words:\n",
    "            tweet[k-1] = '#UNK#'\n",
    "        b_emission = [0 if i is None else i for i in [count_emission.get(key)\\\n",
    "                                         for key in [(state,tweet[k-1]) for state in order]]]\n",
    "        summation = np.multiply(b_transition,b_emission)\n",
    "        beta_list.append(summation)\n",
    "        \n",
    "        return beta(tweet,k-1,count_state,count_emission,count_transistion,order,all_words,beta_list,n_states)\n",
    "    \n",
    "    # Final case\n",
    "    elif k == 1:\n",
    "        return beta_list\n",
    "    \n",
    "    # Recursive case\n",
    "    else:\n",
    "        if tweet[k-1] not in all_words:\n",
    "            tweet[k-1]='#UNK#'\n",
    "        b = []\n",
    "        \n",
    "        for l in range(n_states):\n",
    "            b_transition = [count_transistion.get(key) for key in [(order[l],state) for state in order]]\n",
    "            b_emission = [0 if i is None else i for i in [count_emission.get(key)\\\n",
    "                                         for key in [(order[l],tweet[k-1])]]]\n",
    "            previous_b=beta_list[len(beta_list)-1]\n",
    "            summation = np.multiply(previous_b,b_transition)\n",
    "            b.append(np.sum(b_emission*summation))\n",
    "            \n",
    "        beta_list.append(b)\n",
    "        \n",
    "        return beta(tweet,k-1,count_state,count_emission,count_transistion,order,all_words,beta_list,n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the optimal tag\n",
    "def y_max(tweet,count_state,count_emission,count_transistion,order,all_words,n_states):\n",
    "    y_max_list = []\n",
    "    alpha_list = []\n",
    "    beta_list = []\n",
    "    a_list = alpha(tweet,1,count_state,count_emission,count_transistion,order,all_words,alpha_list,n_states)\n",
    "    b_list = beta(tweet,-1,count_state,count_emission,count_transistion,order,all_words,beta_list,n_states)\n",
    "    b_list_reversed = b_list[::-1]\n",
    "    \n",
    "    for i in range(0,len(a_list)):\n",
    "        summation = np.multiply(a_list[i],b_list_reversed[i])\n",
    "        tag = summation.argmax()\n",
    "        y_max_list.append(order[tag])\n",
    "        \n",
    "    return y_max_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(df, df_test):\n",
    "    # Create dataframes\n",
    "    count_state = Count_State(df)\n",
    "    count_emission = Replace_With_Unk(Count_Emission(df), 3)\n",
    "    all_words = count_emission.reset_index().Observation.values   \n",
    "    count_transistion = Count_Transistion(df)\n",
    "    count_transistion_swap = count_transistion.swaplevel(i = 'State', j = 'J').sort_index()\n",
    "    order = Count_State(df).reset_index().as_matrix()[:-1,0]\n",
    "    n_states = len(order)\n",
    "\n",
    "    # Convert to dictionary\n",
    "    count_state = count_state.to_dict()['Observation']\n",
    "    count_emission = count_emission.to_dict()['emission']\n",
    "    count_transistion = count_transistion.to_dict()['aij']\n",
    "    count_transistion_swap = count_transistion_swap.to_dict()['aij']\n",
    "    \n",
    "    print('Predicting now...')\n",
    "    df_test_size = df_test.reset_index().Tweet.max()+1\n",
    "    predictions = []\n",
    "    s_time = time.time()\n",
    "    \n",
    "    for x in range(df_test_size):\n",
    "        if x % 50 == 0:\n",
    "            print('x:',x,'time:',time.time()-s_time)\n",
    "        tweet = getTweet(df_test,x)\n",
    "        observations = y_max(getTweet(df_test,x),count_state,count_emission,count_transistion,order,all_words,n_states)\n",
    "        predictions = predictions + [\"{} {}\\n\".format(a_, b_) for a_, b_ in zip(tweet[1:-1],observations)] + ['\\n']\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pinar\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transistion Dataframe created.\n",
      "Predicting now...\n",
      "x: 0 time: 0.0\n",
      "x: 50 time: 1.1265294551849365\n",
      "x: 100 time: 2.3797101974487305\n",
      "x: 150 time: 3.526888847351074\n",
      "x: 200 time: 4.835927724838257\n",
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "def __main__(lang): \n",
    "    np.seterr(divide='ignore')#Ignore log zero warnings\n",
    "    df = createDf(lang,'/train')\n",
    "    df_test = createDfDevin(lang,'/dev.in')\n",
    "    \n",
    "    predictions = Predict(df, df_test)\n",
    "    with open('%s/dev.p4.out'%lang, 'w',encoding='utf8') as f:\n",
    "        f.write(''.join(predictions))\n",
    "        print(\"Saved.\")\n",
    "        \n",
    "'''\n",
    "Languages: 'CN' , 'EN', 'FR', 'SG'\n",
    "Change the language below accordingly\n",
    "'''\n",
    "language = 'CN'\n",
    "__main__(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
