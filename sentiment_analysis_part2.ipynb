{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "### Recall that the HMM discussed in class is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hmm_eqn.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def createDf(language):\n",
    "    '''\n",
    "    language = 'CN' , 'EN', 'FR', 'SG'\n",
    "    '''\n",
    "    tweets = []          # A list of all the tweets (Each tweet is a list)\n",
    "    word_count, tweet_count = 0, 0\n",
    "    # Import training data\n",
    "    with open('./' + language + '/train', encoding='utf8') as f:\n",
    "        training_lines = f.readlines()\n",
    "\n",
    "        # For each line in the file\n",
    "        for line in training_lines: \n",
    "\n",
    "            # If line is empty (i.e. we enter a new tweet)\n",
    "            if line in['\\n', '\\r\\n']: # Initialize a new tweet, reset word count\n",
    "                if word_count != 0: #If the previous tweet was not empty, increase tweet count\n",
    "                    tweet_count += 1\n",
    "                word_count = 0\n",
    "\n",
    "            else:\n",
    "                # Remove the spaces in each line\n",
    "                stripped = line.strip().split(\" \")\n",
    "                if len(stripped) == 2:\n",
    "                    if word_count == 0:\n",
    "                        tweets.append([tweet_count, word_count,'None','Start'])\n",
    "                        word_count += 1\n",
    "                    tweets.append([tweet_count, word_count] + stripped)\n",
    "                    word_count += 1\n",
    "                    \n",
    "    df = pd.DataFrame(tweets,columns=['Tweet', 'Word', 'Observation', 'State'])\n",
    "    df = df.set_index(['Tweet', 'Word'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDfDevin(language, file):\n",
    "    '''\n",
    "    language = 'CN' , 'EN', 'FR', 'SG'\n",
    "    '''\n",
    "    # A list of all the tweets (Each tweet is a list)\n",
    "    tweets = []          \n",
    "    word_count, tweet_count = 0, 0\n",
    "    \n",
    "    # Import training data\n",
    "    with open('./' + language + file, encoding='utf8') as f:\n",
    "        training_lines = f.readlines()\n",
    "\n",
    "        # For each line in the file\n",
    "        for line in training_lines: \n",
    "\n",
    "            # If line is empty (i.e. we enter a new tweet)\n",
    "            if line in['\\n', '\\r\\n']: # Initialize a new tweet, reset word count\n",
    "                if word_count != 0: #If the previous tweet was not empty, increase tweet count\n",
    "                    tweet_count += 1\n",
    "                word_count = 0\n",
    "\n",
    "            else:\n",
    "                # Remove the spaces in each line\n",
    "                stripped = line.strip().split(\" \")\n",
    "                if len(stripped) == 1:\n",
    "                    if word_count == 0:\n",
    "                        tweets.append([tweet_count, word_count,'None'])\n",
    "                        word_count += 1\n",
    "                    tweets.append([tweet_count, word_count] + stripped)\n",
    "                    word_count += 1\n",
    "    df = pd.DataFrame(tweets,columns=['Tweet', 'Word', 'Observation'])\n",
    "    df = df.set_index(['Tweet', 'Word'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweet(df, tweetNumber):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    df: dataframe of all tweets\n",
    "    tweetNumber: which tweet to access and extract\n",
    "\n",
    "    Output:\n",
    "    obs_list: list of observations for a specified tweet \n",
    "    \"\"\"\n",
    "    df_resetindex = df.reset_index()\n",
    "    tweet_df = df_resetindex.loc[df_resetindex['Tweet'] == tweetNumber]\n",
    "    \n",
    "    # Convert tweet dataframe to a list\n",
    "    tweet_list = tweet_df.values.T.tolist()\n",
    "    \n",
    "    # Append a None at the end of observation to account for 'Stop' state\n",
    "    obs_list = tweet_list[2]\n",
    "    obs_list.append('None')\n",
    "    \n",
    "    # returns a list of observations\n",
    "    return obs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain count of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Count_State(df):\n",
    "    '''\n",
    "    Get Count(i) and Count(j)\n",
    "    '''\n",
    "    states_count = df.groupby('State').count()\n",
    "    return states_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Transistion(df, count_state):\n",
    "    transistion = df.copy()\n",
    "    transistion['J'] = transistion['State']\n",
    "    transistion['J'] = transistion['J'].shift(-1)\n",
    "    transistion['J'].loc[transistion['J'] == 'Start'] = 'Stop'\n",
    "    transistion['J'].loc[pd.isnull(transistion['J'])] = 'Stop'\n",
    "    count_transistion = transistion.groupby(['State','J']).count()\n",
    "    \n",
    "    # Create full table of transistion permutations\n",
    "    states = Count_State(df).reset_index().as_matrix()[:-1,0]\n",
    "    length = states.shape[0] + 1\n",
    "    start = np.reshape(np.concatenate((['Start'],states)),(1,-1))\n",
    "    stop = np.reshape(np.concatenate((states,['Stop'])),(1,-1))\n",
    "    states = np.vstack((np.repeat(start,length),np.ravel(np.repeat(stop,length,axis=0)))).T\n",
    "    states = pd.DataFrame(states, columns=['State','J'])\n",
    "    states['Observation'] = 0\n",
    "    states = states.set_index(['State','J'])\n",
    "    count_transistion = states.join(count_transistion, how='left', lsuffix='2').drop('Observation2', axis=1).fillna(0)\n",
    "    \n",
    "    # Compute transistion probabilities\n",
    "    count_transistion = count_transistion.join(count_state, lsuffix='_trans', rsuffix='_state')\n",
    "    count_transistion['aij'] = count_transistion['Observation_trans'] / count_transistion['Observation_state']\n",
    "    \n",
    "    return count_transistion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5 pts) Write a function that estimates the emission parameters from the training set using MLE (maximum likelihood estimation):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/MLE.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Emission(df):\n",
    "    # Create a column 'Counts' filled with 1's\n",
    "    df[\"Count\"] = 1\n",
    "    \n",
    "    # Dataframe manipulation to obtain emission count\n",
    "    emission_count = df.groupby(['State','Observation'],).count().reset_index(level = 'Observation')\n",
    "    emission_count = emission_count.join(Count_State(df), rsuffix = '_State')\n",
    "    emission_count = emission_count.drop('Observation_State',axis=1)\n",
    "    emission_count[\"emission\"] = emission_count['Count'] / emission_count['Count_State']\n",
    "    emission_count = emission_count.drop('Count_State',axis=1)\n",
    "    \n",
    "    return emission_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (10 pts) During the testing phase, if the word does not appear in the “modified training set”, we replace that word with #UNK# as well. Set k to 3, implement this fix into your function for computing the emission parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Replace_With_Unk(df, k):\n",
    "    emission_count = df.copy()\n",
    "    drop_table = emission_count.groupby(['Observation'],).sum()\n",
    "    drop_table = drop_table.loc[drop_table['Count'] < k].reset_index()\n",
    "    emission_count['Observation'].loc[emission_count['Observation'].isin(drop_table['Observation'])] = '#UNK#'\n",
    "    emission_count = emission_count.groupby(['State','Observation'],).sum()\n",
    "    \n",
    "    return emission_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (10 pts) Implement a simple sentiment analysis system that produces the tag y* = arg max e(x|y) for each word x in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToDataFrame(df):\n",
    "    index = df.groupby(\"Observation\").idxmax()\n",
    "    obs_list = index.as_matrix()\n",
    "    index_list = []\n",
    "    \n",
    "    for i in range(0,len(obs_list)):\n",
    "        index_list.append(obs_list[i][1])\n",
    "        \n",
    "    tags = pd.DataFrame(index_list,columns=['Observation', 'State']).set_index('Observation')\n",
    "    \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictOutput(language, tags):\n",
    "    test_pred = []\n",
    "\n",
    "    with open('./' + language + '/dev.in', encoding='utf8') as test:\n",
    "\n",
    "        lines = test.readlines()\n",
    "        test_lines =[]\n",
    "        for line in lines:\n",
    "            line = line.strip(\"\\n\")\n",
    "            test_lines.append(line)\n",
    "\n",
    "    for i in test_lines:\n",
    "        try:\n",
    "            if i == '':\n",
    "                x = ''\n",
    "            else:\n",
    "                x = tags.loc[i]\n",
    "        except KeyError:\n",
    "            x = tags.loc[\"#UNK#\"]\n",
    "\n",
    "        for j in x:\n",
    "            x = j\n",
    "\n",
    "        temp = [i,x]\n",
    "        test_pred.append(temp)\n",
    "\n",
    "    tests = pd.DataFrame(test_pred,columns=['Observation',\"Predictions\"])\n",
    "    tests.to_csv(r'./' + language + '/dev.p2.out', header=None, index=None, sep=' ', mode='a',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __main__(language):\n",
    "    df = createDf(language)\n",
    "    count_state = Count_State(df)\n",
    "    count_transistion = Count_Transistion(df, count_state)\n",
    "    emission_count = Count_Emission(df)\n",
    "    emission_count = Replace_With_Unk(emission_count, 3)\n",
    "    emission_count = emission_count.swaplevel(i = 'Observation', j = 'State' ).sortlevel()\n",
    "    tags = convertToDataFrame(emission_count)\n",
    "        \n",
    "    predictOutput(language, tags)\n",
    "    print(\"File is saved\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pinar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: sortlevel is deprecated, use sort_index(level= ...)\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is saved\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None  # Turn off warning message\n",
    "\n",
    "'''\n",
    "Languages: 'CN' , 'EN', 'FR', 'SG'\n",
    "Change the language below accordingly\n",
    "'''\n",
    "language = 'EN' \n",
    "__main__(language)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
