{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Modified Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Function to create lowercase training set\n",
    "def createCleanTrain(language, file):\n",
    "    '''\n",
    "    language = 'CN' , 'EN', 'FR', 'SG'\n",
    "    '''\n",
    "    \n",
    "    original = open('./' + language + file, \"r\", encoding='utf8')\n",
    "    output = open('./' + language + file + \"_clean\", \"w+\", encoding='utf8')\n",
    "\n",
    "    \n",
    "    # Import training data\n",
    "    with original as f, output as g:\n",
    "            \n",
    "        for lf in f:\n",
    "           # if line is not empty\n",
    "            if lf != \"\\n\":\n",
    "                try:\n",
    "                    o, s = lf.strip().split(\" \")\n",
    "                except:\n",
    "                    o = lf.strip().split(\" \")[0]\n",
    "                    s = None\n",
    "                    \n",
    "                new_o = o.lower()\n",
    "                if s is None:\n",
    "                    g.write(new_o + \"\\n\")\n",
    "                else:\n",
    "                    g.write(\" \".join([new_o, s]) + \"\\n\")\n",
    "                    \n",
    "            # else if line if empty\n",
    "            else:\n",
    "                g.write(lf)\n",
    "                \n",
    "        f.close()\n",
    "        g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create lowercase dev.in\n",
    "def createCleanDevIn(language, file):\n",
    "    '''\n",
    "    language = 'CN' , 'EN', 'FR', 'SG'\n",
    "    '''\n",
    "    \n",
    "    original = open('./' + language + file, \"r\", encoding='utf8')\n",
    "    output = open('./' + language + file[0:4] + \"_clean.in\", \"w\", encoding='utf8')\n",
    "\n",
    "    \n",
    "    # Import training data\n",
    "    with original as f, output as g:\n",
    "            \n",
    "        for lf in f:\n",
    "           # if line is not empty\n",
    "            if lf != \"\\n\":\n",
    "                try:\n",
    "                    o, s = lf.strip().split(\" \")\n",
    "                except:\n",
    "                    o = lf.strip().split(\" \")[0]\n",
    "                    s = None\n",
    "                    \n",
    "                new_o = o.lower()\n",
    "                if s is None:\n",
    "                    g.write(new_o + \"\\n\")\n",
    "                else:\n",
    "                    g.write(\" \".join([new_o, s]) + \"\\n\")\n",
    "                    \n",
    "            # else if line if empty\n",
    "            else:\n",
    "                g.write(lf)\n",
    "                \n",
    "        f.close()\n",
    "        g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "def createDf(language,file):\n",
    "    '''\n",
    "    language = 'CN' , 'EN', 'FR', 'SG'\n",
    "    '''\n",
    "    tweets = []          # A list of all the tweets (Each tweet is a list)\n",
    "    word_count, tweet_count = 0, 0\n",
    "    # Import training data\n",
    "    with open('./' + language + file, encoding='utf8') as f:\n",
    "        training_lines = f.readlines()\n",
    "\n",
    "        # For each line in the file\n",
    "        for line in training_lines: \n",
    "\n",
    "            # If line is empty (i.e. we enter a new tweet)\n",
    "            if line in['\\n', '\\r\\n']: # Initialize a new tweet, reset word count\n",
    "                if word_count != 0: #If the previous tweet was not empty, increase tweet count\n",
    "                    tweet_count += 1\n",
    "                word_count = 0\n",
    "\n",
    "            else:\n",
    "                # Remove the spaces in each line\n",
    "                stripped = line.strip().split(\" \")\n",
    "                if len(stripped) == 2:\n",
    "                    if word_count == 0:\n",
    "                        tweets.append([tweet_count, word_count,'None','Start'])\n",
    "                        word_count += 1\n",
    "                    tweets.append([tweet_count, word_count] + stripped)\n",
    "                    word_count += 1\n",
    "    df = pd.DataFrame(tweets,columns=['Tweet', 'Word', 'Observation', 'State'])\n",
    "    df = df.set_index(['Tweet', 'Word'])\n",
    "    print('Training dataframe created.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dev.in dataframe\n",
    "def createDfDevin(language,file):\n",
    "    '''\n",
    "    language = 'CN' , 'EN', 'FR', 'SG'\n",
    "    '''\n",
    "    tweets = []          # A list of all the tweets (Each tweet is a list)\n",
    "    word_count, tweet_count = 0, 0\n",
    "    # Import training data\n",
    "    with open('./' + language + file, encoding='utf8') as f:\n",
    "        training_lines = f.readlines()\n",
    "\n",
    "        # For each line in the file\n",
    "        for line in training_lines: \n",
    "\n",
    "            # If line is empty (i.e. we enter a new tweet)\n",
    "            if line in['\\n', '\\r\\n']: # Initialize a new tweet, reset word count\n",
    "                if word_count != 0: #If the previous tweet was not empty, increase tweet count\n",
    "                    tweet_count += 1\n",
    "                word_count = 0\n",
    "\n",
    "            else:\n",
    "                # Remove the spaces in each line\n",
    "                stripped = line.strip().split(\" \")\n",
    "                if len(stripped) == 1:\n",
    "                    if word_count == 0:\n",
    "                        tweets.append([tweet_count, word_count,'None'])\n",
    "                        word_count += 1\n",
    "                    tweets.append([tweet_count, word_count] + stripped)\n",
    "                    word_count += 1\n",
    "    df = pd.DataFrame(tweets,columns=['Tweet', 'Word', 'Observation'])\n",
    "    df = df.set_index(['Tweet', 'Word'])\n",
    "    print('Testing dataframe created.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweet(df, tweetNumber):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    df: dataframe of all tweets\n",
    "    tweetNumber: which tweet to access and extract\n",
    "\n",
    "    Output:\n",
    "    obs_list: list of observations for a specified tweet \n",
    "    \"\"\"\n",
    "    df_resetindex = df.reset_index()\n",
    "    tweet_df = df_resetindex.loc[df_resetindex['Tweet'] == tweetNumber]\n",
    "    \n",
    "    # Convert tweet dataframe to a list\n",
    "    tweet_list = tweet_df.values.T.tolist()\n",
    "    \n",
    "    # Append a None at the end of observation to account for 'Stop' state\n",
    "    obs_list = tweet_list[2]\n",
    "    obs_list.append('None')\n",
    "    \n",
    "    return obs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweetLabel(df, tweetNumber):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    df: dataframe of all tweets\n",
    "    tweetNumber: which tweet to access and extract\n",
    "\n",
    "    Output:\n",
    "    obs_list: list of observations for a specified tweet \n",
    "    \"\"\"\n",
    "    df_resetindex = df.reset_index()\n",
    "    tweet_df = df_resetindex.loc[df_resetindex['Tweet'] == tweetNumber]\n",
    "    \n",
    "    # Convert tweet dataframe to a list\n",
    "    tweet_list = tweet_df.values.T.tolist()\n",
    "    \n",
    "    # Append a None at the end of observation to account for 'Stop' state\n",
    "    obs_list = tweet_list[3]\n",
    "    \n",
    "    return obs_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_State(df):\n",
    "    '''\n",
    "    Get Count(i) and Count(j)\n",
    "    '''\n",
    "    states_counter = df.groupby('State').count()\n",
    "    return states_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Emission Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Emission(df):\n",
    "    df = df.copy()\n",
    "    df[\"Count\"] = 1\n",
    "    \n",
    "    count_emission = df.groupby(['State','Observation'],).count().reset_index(level = 'Observation')\n",
    "    count_emission = count_emission.join(Count_State(df), rsuffix = '_State')\n",
    "    count_emission = count_emission.drop('Observation_State',axis=1)    \n",
    "    count_emission[\"emission\"] = count_emission['Count'] / count_emission['Count_State']\n",
    "    count_emission = count_emission.drop('Count_State',axis=1)\n",
    "    print(\"Emission Dataframe created.\")\n",
    "    return count_emission\n",
    "\n",
    "def Replace_With_Unk(df, k):\n",
    "    emission_count = df.copy()\n",
    "    drop_table = emission_count.groupby(['Observation'],).sum()\n",
    "    drop_table = drop_table.loc[drop_table['Count'] < k].reset_index()\n",
    "    emission_count['Observation'].loc[emission_count['Observation'].isin(drop_table['Observation'])] = '#UNK#'\n",
    "    emission_count = emission_count.groupby(['State','Observation'],).sum()\n",
    "    \n",
    "    return emission_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5 pts) Write a function that estimates the transition parameters from the training set using MLE (maximum likelihood estimation):\n",
    "\n",
    "#### Please make sure the following special cases are also considered: q(STOP|yn) and q(y1|START)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/mle2.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Transistion(df):\n",
    "    transistion = df.copy()\n",
    "    transistion['J'] = transistion['State']\n",
    "    transistion['J'] = transistion['J'].shift(-1)\n",
    "    transistion['J'].loc[transistion['J'] == 'Start'] = 'Stop'\n",
    "    transistion['J'].loc[pd.isnull(transistion['J'])] = 'Stop'\n",
    "    count_transistion = transistion.groupby(['State','J']).count()\n",
    "    \n",
    "    #Create Full table of transistion permutations\n",
    "    states = Count_State(df).reset_index().as_matrix()[:-1,0]\n",
    "    length = states.shape[0] + 1\n",
    "    start = np.reshape(np.concatenate((['Start'],states)),(1,-1))\n",
    "    Stop = np.reshape(np.concatenate((states,['Stop'])),(1,-1))\n",
    "    states = np.vstack((np.repeat(start,length),np.ravel(np.repeat(Stop,length,axis=0)))).T\n",
    "    states = pd.DataFrame(states, columns=['State','J'])\n",
    "    states['Observation'] = 0\n",
    "    states = states.set_index(['State','J'])\n",
    "    count_transistion = states.join(count_transistion, how= 'left', lsuffix= '2').drop('Observation2', axis = 1).fillna(0)\n",
    "    \n",
    "    #Compute transistion probabilities\n",
    "    count_transistion = count_transistion.join(Count_State(df), lsuffix='_trans', rsuffix='_state')\n",
    "    count_transistion['aij'] = count_transistion['Observation_trans'] / count_transistion['Observation_state']\n",
    "    print(\"Transistion Dataframe created.\")\n",
    "    \n",
    "    return count_transistion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Viterbi_W (global_trigram, global_weights, tweet, df, word_list, count_emission, count_transistion, order, prob_max=None, arg_max=None, level = 1, train = True):\n",
    "    '''\n",
    "    @@@This Viterbi is modified for weights@@@\n",
    "    tweet    : Tweet formatted into list of words\n",
    "    prob_max : Max probability of layer\n",
    "    arg_max  : Max argument of layer\n",
    "    level    : Current layer\n",
    "    '''\n",
    "    # Base case\n",
    "    if level == 1:\n",
    "        pi = np.ones(len(order))\n",
    "        transmission = [count_transistion.get(key) for key in [('Start',next_state) for next_state in order]]\n",
    "        emission = [0 if i is None else i for i in [count_emission.get(key)\\\n",
    "                                         for key in [(state,tweet[level]) for state in order]]]\n",
    "        new_prob_max = pi + np.log(transmission) + np.log(emission) #elementwise log addition\n",
    "        new_arg_max = np.full((len(order)),'Start')\n",
    "        new_prob_max += getWordWeights(tweet[level], order, global_weights) #update with global weights\n",
    "\n",
    "    else:\n",
    "        new_prob_max = []\n",
    "        new_arg_max = []\n",
    "        pi = prob_max\n",
    "        \n",
    "        # Final case (len(new_argmax) = len(new_max) = 1)\n",
    "        if level == tweet.shape[0]-1:\n",
    "            transmission = [count_transistion.get(key) for key in [(prev_state,'Stop') for prev_state in order]]\n",
    "            emission = np.ones(len(order)) #Emission in STOP state = 1\n",
    "            intermediate = pi + np.log(transmission) + np.log(emission) #elementwise log addition\n",
    "                        \n",
    "            new_prob_max = intermediate.max()\n",
    "            new_arg_max = order[intermediate.argmax()] #Best previous state\n",
    "        \n",
    "        # Recursive case (len(new_argmax) = len(new_max) = # of possible states)\n",
    "        else:\n",
    "            emission_all = [0 if i is None else i for i in [count_emission.get(key)\\\n",
    "                             for key in [(state,tweet[level]) for state in order]]]\n",
    "            for i in range(len(order)):\n",
    "                transmission = [count_transistion.get(key) for key in [(prev_state,order[i]) for prev_state in order]]\n",
    "                emission = np.full(7,emission_all[i])\n",
    "                intermediate = pi + np.log(transmission) + np.log(emission) #elementwise log addition\n",
    "\n",
    "                new_prob_max.append(intermediate.max())\n",
    "                new_arg_max.append(order[intermediate.argmax()]) #Best previous state\n",
    "            \n",
    "            new_prob_max = np.asarray(new_prob_max)\n",
    "            new_arg_max = np.asarray(new_arg_max)\n",
    "            \n",
    "            new_prob_max += getWordWeights(tweet[level], order, global_weights) #update with global weights\n",
    "            \n",
    "            if level >= 3:\n",
    "                state_trans_a = {a_:b_ for a_,b_ in zip(order,arg_max)}\n",
    "                state_trans_b = {a_:b_ for a_,b_ in zip(order,new_arg_max)}\n",
    "                tristate_list = []\n",
    "                for c in list(state_trans_b.keys()):\n",
    "                    b = state_trans_b[c]\n",
    "                    a = state_trans_a[b]\n",
    "                    tristate_list.append([a,b,c])\n",
    "                new_prob_max += getTrigramWeights(tristate_list, global_trigram)\n",
    "            \n",
    "    # Final case\n",
    "    if level == tweet.shape[0]-1: #Recursion termination (reached nth term)\n",
    "        return ([arg_max[np.where(order == new_arg_max)[0].item()],new_arg_max])\n",
    "    \n",
    "    else: #Need further recursion, increment level(layer)\n",
    "        \n",
    "        #Forward Propagation\n",
    "        path = Viterbi_W (global_trigram, global_weights, tweet, df, word_list, count_emission, count_transistion, order, new_prob_max, new_arg_max, level+1, train)\n",
    "        \n",
    "        #Backward Propagation\n",
    "        #When backward propagation finishes, append stop\n",
    "        if len(np.where(order == path[0])[0]) == 0: \n",
    "            return path +['Stop']\n",
    "        \n",
    "        #If yet to finish, concat newly discovered state to path\n",
    "        else:\n",
    "            return [arg_max[np.where(order == path[0])[0].item()]] + path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Trigrams & tag-word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trigram objects\n",
    "def initTagTrigram(order):\n",
    "    order_permute = np.array(np.meshgrid(order, order, order)).T.reshape(-1,3)\n",
    "    return {' '.join(word): 0 for word in order_permute}\n",
    "\n",
    "# Create tag-word pairs\n",
    "def initTagWordPair(order, words):\n",
    "    order_permute = np.array(np.meshgrid(order, words)).T.reshape(-1,2)\n",
    "    tagWordPair = {' '.join(word): 0 for word in order_permute}\n",
    "    return tagWordPair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating of Trigrams & tag-word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Update trigrams\n",
    "def updateTagTrigram(actual_states, predicted_states, tagTrigram, learning_rate = 1):\n",
    "    assert(len(predicted_states) == len(actual_states)), \"Actual and predicted states are of different length\"\n",
    "    \n",
    "    # For actual states\n",
    "    for i in range(len(actual_states) - 2):\n",
    "        trigram = actual_states[i:i+3]\n",
    "        trigram = ' '.join(trigram)\n",
    "        tagTrigram[trigram] += learning_rate\n",
    "    \n",
    "    # For predicted states\n",
    "    for i in range(len(predicted_states) - 2):\n",
    "        trigram = predicted_states[i:i+3]\n",
    "        trigram = ' '.join(trigram)\n",
    "        tagTrigram[trigram] -= learning_rate\n",
    "    return tagTrigram\n",
    "\n",
    "\n",
    "def updateTagWordPair(tweet, actual_states, predicted_states, tagWordPair, learning_rate = 1):\n",
    "    assert(len(actual_states) == len(predicted_states) == len(tweet)),\"Length of states/words are not equal %d, %d, %d\"\n",
    "    \n",
    "    # iterate through each actual state and words\n",
    "    actual_pairs = [\"{} {}\".format(a_, b_) for a_, b_ in zip(actual_states, tweet)]\n",
    "\n",
    "    # iterate through each predicted state and words\n",
    "    predicted_pairs = [\"{} {}\".format(a_, b_) for a_, b_ in zip(predicted_states, tweet)]\n",
    "\n",
    "    # Increment score by 1 if actual tagword pair\n",
    "    for x in actual_pairs:\n",
    "        tagWordPair[x] += learning_rate\n",
    "        \n",
    "    # Decrement score by 1 if actual tagword pair\n",
    "    for x in predicted_pairs:\n",
    "        tagWordPair[x] -= learning_rate\n",
    "    \n",
    "    return tagWordPair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining trigram weights & word weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrigramWeights(tristate_list, global_trigram):\n",
    "    order_permute = [\" \".join(x) for x in tristate_list]\n",
    "    return np.asarray([global_trigram[key] for key in order_permute])\n",
    "\n",
    "\n",
    "def getWordWeights(word, order, weights):\n",
    "    order_permute = [\"{} {}\".format(a_, b_) for a_, b_ in zip(order, [word]*7)]\n",
    "    return np.asarray([weights[key] for key in order_permute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perceptron(tweet, label, global_trigram, global_weights, df, word_list, count_emission, count_transistion, order, iteration=20, trigram_rate=1, word_rate=1):\n",
    "    tweet = tweet.tolist()\n",
    "    for i in range(1,len(tweet)-1):\n",
    "        if tweet[i] not in word_list: #These are Start Stop states containing None\n",
    "            tweet[i] = '#UNK#'\n",
    "    tweet = np.asarray(tweet)\n",
    "    count=0\n",
    "    for i in range(iteration):\n",
    "        predicted_label = Viterbi_W(global_trigram, global_weights, tweet, df, word_list, count_emission, count_transistion, order)[1:-1]\n",
    "        count= count + 1\n",
    "        if not np.array_equal(np.asarray(predicted_label), label):\n",
    "            global_trigram = updateTagTrigram(label, predicted_label, global_trigram, trigram_rate)\n",
    "            global_weights = updateTagWordPair(tweet[1:-1], label, predicted_label, global_weights, word_rate)\n",
    "        else:\n",
    "            break\n",
    "    return global_trigram, global_weights\n",
    "\n",
    "\n",
    "def TrainPerceptron(lang):\n",
    "    np.seterr(divide='ignore') #Ignore log zero warnings\n",
    "    createCleanTrain(lang, '/train')\n",
    "    createCleanDevIn(lang, '/dev.in')\n",
    "    df = createDf(lang,'/train_clean')\n",
    "    df_test = createDfDevin(lang,'/dev_clean.in')\n",
    "    \n",
    "    count_state = Count_State(df)\n",
    "    count_emission = Replace_With_Unk(Count_Emission(df), 2)\n",
    "    word_list = count_emission.reset_index().Observation.values\n",
    "    count_transistion = Count_Transistion(df)\n",
    "\n",
    "    count_transistion_swap = count_transistion.swaplevel(i = 'State', j = 'J').sort_index()\n",
    "    order = Count_State(df).reset_index().as_matrix()[:-1,0]\n",
    "    \n",
    "    count_state = count_state.to_dict()['Observation']\n",
    "    count_emission = count_emission.to_dict()['emission']\n",
    "    count_transistion = count_transistion.to_dict()['aij']\n",
    "    count_transistion_swap = count_transistion_swap.to_dict()['aij']\n",
    "    \n",
    "    global_weights = initTagWordPair(order, word_list)\n",
    "    global_trigram = initTagTrigram(order)\n",
    "    print('Training Perceptron...')\n",
    "    df_train_size = df_test.reset_index().Tweet.max()+1\n",
    "    s_time = time.time()\n",
    "    \n",
    "    # Use training set to train Perceptron\n",
    "    for x in range(df_train_size):\n",
    "        if x%100==0:\n",
    "            print('Tweet number:',x,'time:',time.time()-s_time)\n",
    "        tweet_x = np.asarray(getTweet(df,x))\n",
    "        tweet_y = np.asarray(getTweetLabel(df, x))\n",
    "        parameters = {'EN':{'iteration':200 , 'trigram_rate':0.00001, 'word_rate':0.0001 },'FR':{'iteration':50 , 'trigram_rate':0.001, 'word_rate':0.00001}}\n",
    "\n",
    "        global_trigram, global_weights = Perceptron(tweet_x, tweet_y, global_trigram, global_weights, df, word_list, count_emission, count_transistion, order, iteration=parameters.get(lang).get('iteration'), trigram_rate=parameters.get(lang).get('trigram_rate'), word_rate=parameters.get(lang).get('word_rate'))\n",
    "    \n",
    "    # Use dev.in to predict the labels\n",
    "    df_test_size = df_test.reset_index().Tweet.max()+1\n",
    "    predictions = []\n",
    "    pred_list = []\n",
    "    s_time = time.time()\n",
    "    \n",
    "    # Iterations for prediction\n",
    "    print('Predicting...')\n",
    "    for x in range(df_test_size):\n",
    "        if x%100==0:\n",
    "            print('Tweet number:',x,'time:',time.time()-s_time)\n",
    "        tweet = np.asarray(getTweet(df_test,x))\n",
    "        tweet = tweet.tolist()\n",
    "        for i in range(1,len(tweet)-1):\n",
    "            if tweet[i] not in word_list: #These are Start Stop states containing None\n",
    "                tweet[i] = '#UNK#'\n",
    "        tweet = np.asarray(tweet)\n",
    "        observations = Viterbi_W(global_trigram, global_weights, tweet, df, word_list, count_emission, count_transistion, order, train = False)[1:-1]\n",
    "        predictions = predictions + [\"{} {}\\n\".format(a_, b_) for a_, b_ in zip(tweet[1:-1],observations)] + ['\\n']\n",
    "\n",
    "        pred_list = pred_list + [x for x in zip(tweet[1:-1],observations)]\n",
    "        \n",
    "    with open('%s/dev.p5.out'%lang, 'w',encoding='utf8') as f:\n",
    "        f.write(''.join(predictions))\n",
    "        print(\"Saved.\")\n",
    "    return global_weights, global_trigram, predictions, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataframe created.\n",
      "Testing dataframe created.\n",
      "Emission Dataframe created.\n",
      "Transistion Dataframe created.\n",
      "Training Perceptron...\n",
      "Tweet number: 0 time: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pinar\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet number: 100 time: 15.412178039550781\n",
      "Tweet number: 200 time: 31.302725791931152\n",
      "Predicting...\n",
      "Tweet number: 0 time: 0.0009999275207519531\n",
      "Tweet number: 100 time: 0.799354076385498\n",
      "Tweet number: 200 time: 1.4084093570709229\n",
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "def __main__(language):\n",
    "    global_weights, global_trigram, predictions, pred_list = TrainPerceptron(language)\n",
    "    \n",
    "language = 'FR'\n",
    "__main__(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
